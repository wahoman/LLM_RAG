{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSTLabs\\miniconda3\\envs\\wahoman\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"beomi/OPEN-SOLAR-KO-10.7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "은행금리에 대해 설명해줘 [/INST]\n",
      "은행금리는 은행이 예금자에게 예금에 대한 대가로 지급하는 이자율이다. 은행은 예금자에게 이자를 지급하는 대신 예금자로부터 예금에 대한 대가로 이자를 지급받는다. 은행은 예금자에게 지급한 이자를 예금자에게 지급받은 이자로 충당한다. 은행이 예금자에게 지급하는 이자를 예금금리라고 하고, 은행이 예금자로부터 지급받는 이자를 대출금리라고 한다. 은행은 예금금리와 대출금리의 차이를 통해 수익을 창출한다\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"은행금리에 대해 설명해줘\"}\n",
    "]\n",
    "\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs,\n",
    "    max_new_tokens=100,  # 응답 길이 제한\n",
    "    num_return_sequences=1,  # 하나의 응답만 생성\n",
    "    temperature=0.1,  # 응답의 다양성 조절\n",
    "    do_sample=True\n",
    ")\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install시 utf-8, ansi 관련 오류날 경우 필요한 코드\n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=300\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### [INST]\n",
    "Instruction: Answer the question based on your knowledge.\n",
    "Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question}\n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "koplatyi_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create prompt from prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain\n",
    "llm_chain = LLMChain(llm=koplatyi_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"D:/여형구/test.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(texts, hf)\n",
    "retriever = db.as_retriever(\n",
    "                            search_type=\"similarity\",\n",
    "                            search_kwargs={'k': 3}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주어진 근거: - iv -<그림24>라벨링데이터예시 29\n",
      "<그림25>VisualStudioCode환경에서YOLOv5모델학습30\n",
      "<그림26>YOLOv5x모델학습결과(그림) 32\n",
      "<그림27>학습모델의F1_curve(좌상),P_curve(우상),PR_curve(좌\n",
      "하),R_curve(우하) 33\n",
      "<그림28>True예시(좌),False중클래스만잘못예측한예시(우)34\n",
      "<그림29>테스트데이터셋디텍팅결과(그림) 35\n",
      "<그림30>실시간Detecting구현예시 36 / 출처: D:/여형구/test.pdf - 6 \n",
      "\n",
      "\n",
      "주어진 근거: - 21 -3)YOLOv5[23]\n",
      "YOLO는YouOnlyLookOnce의약자로객체탐지분야에서가장많이알\n",
      "려진모델이다.객체탐지모델에는두가지접근방식이존재하는데예측결\n",
      "과를도출하기까지몇번의단계를거치는지에따라One-Stage방식과\n",
      "Two-Stage방식으로나누어진다.YOLO는이름그대로One-Stage방식으로\n",
      "객체를탐지하는모델로객체의경계상자(boundingbox)와클래스레이블\n",
      "(classlabel)을직접예측하는모델이다.방식의특성상구현이간단하고속도\n",
      "가빠르다는장점이있어YOLO모델이실시간객체탐지가가능하게한다.\n",
      "그림16One-StagevsTwo-Stage방식비교16)\n",
      "YOLO모델은이미지가입력되면우선가로세로를동일한그리드영역으\n",
      "로나눈다.그후그리드영역에대해서어디에객체가존재하는지바운딩박\n",
      "스와해당박스에대한신뢰도점수를예측한다.이와동시에객체의종류를\n",
      "예측하는Classification작업이진행된다.최종적으로정확도가높은박스들만\n",
      "남기고NMS(Non-Maximum Suppression)[22]알고리즘을이용해선별하여 / 출처: D:/여형구/test.pdf - 29 \n",
      "\n",
      "\n",
      "주어진 근거: - 30 -3)모델학습환경준비\n",
      "본연구에서는적은양의데이터를가지고최선의결과를도출하기위해\n",
      "YOLOv5의4가지모델중가장크고무거운YOLOv5x모델을사용하였다.\n",
      "또한,학습파라미터를표6과같이설정하여학습을진행하였다.\n",
      "종류 값\n",
      "LearningRate(학습률) 0.01\n",
      "BatchSize(배치크기) 8\n",
      "Epochs(에폭) 50\n",
      "Optimizer(옵티마이저) Adam\n",
      "WeightDecay(가중치감소) 0.0005\n",
      "Momentum(모멘텀) 0.937\n",
      "InputImageSize(입력이미지크기) 640\n",
      "표6모델학습파라미터\n",
      "폭발물탐지모델은3080TiGPUCUDA환경에서VisualStudioCode를사\n",
      "용하였고,메모리(RAM)의학습환경에서동작하였다.\n",
      "종류 항목\n",
      "모델 YOLOv5x\n",
      "프레임워크 VisualStudioCode/Python&Pytorch\n",
      "GPU NvidiaRTX3080Ti\n",
      "RAM 32GB\n",
      "표7모델학습환경 / 출처: D:/여형구/test.pdf - 38 \n",
      "\n",
      "\n",
      "\n",
      "답변: \n",
      "### [INST]\n",
      "Instruction: Answer the question based on your knowledge.\n",
      "Here is context to help:\n",
      "\n",
      "[Document(page_content='- iv -<그림24>라벨링데이터예시 29\\n<그림25>VisualStudioCode환경에서YOLOv5모델학습30\\n<그림26>YOLOv5x모델학습결과(그림) 32\\n<그림27>학습모델의F1_curve(좌상),P_curve(우상),PR_curve(좌\\n하),R_curve(우하) 33\\n<그림28>True예시(좌),False중클래스만잘못예측한예시(우)34\\n<그림29>테스트데이터셋디텍팅결과(그림) 35\\n<그림30>실시간Detecting구현예시 36', metadata={'source': 'D:/여형구/test.pdf', 'page': 6}), Document(page_content='- 21 -3)YOLOv5[23]\\nYOLO는YouOnlyLookOnce의약자로객체탐지분야에서가장많이알\\n려진모델이다.객체탐지모델에는두가지접근방식이존재하는데예측결\\n과를도출하기까지몇번의단계를거치는지에따라One-Stage방식과\\nTwo-Stage방식으로나누어진다.YOLO는이름그대로One-Stage방식으로\\n객체를탐지하는모델로객체의경계상자(boundingbox)와클래스레이블\\n(classlabel)을직접예측하는모델이다.방식의특성상구현이간단하고속도\\n가빠르다는장점이있어YOLO모델이실시간객체탐지가가능하게한다.\\n그림16One-StagevsTwo-Stage방식비교16)\\nYOLO모델은이미지가입력되면우선가로세로를동일한그리드영역으\\n로나눈다.그후그리드영역에대해서어디에객체가존재하는지바운딩박\\n스와해당박스에대한신뢰도점수를예측한다.이와동시에객체의종류를\\n예측하는Classification작업이진행된다.최종적으로정확도가높은박스들만\\n남기고NMS(Non-Maximum Suppression)[22]알고리즘을이용해선별하여', metadata={'source': 'D:/여형구/test.pdf', 'page': 29}), Document(page_content='- 30 -3)모델학습환경준비\\n본연구에서는적은양의데이터를가지고최선의결과를도출하기위해\\nYOLOv5의4가지모델중가장크고무거운YOLOv5x모델을사용하였다.\\n또한,학습파라미터를표6과같이설정하여학습을진행하였다.\\n종류 값\\nLearningRate(학습률) 0.01\\nBatchSize(배치크기) 8\\nEpochs(에폭) 50\\nOptimizer(옵티마이저) Adam\\nWeightDecay(가중치감소) 0.0005\\nMomentum(모멘텀) 0.937\\nInputImageSize(입력이미지크기) 640\\n표6모델학습파라미터\\n폭발물탐지모델은3080TiGPUCUDA환경에서VisualStudioCode를사\\n용하였고,메모리(RAM)의학습환경에서동작하였다.\\n종류 항목\\n모델 YOLOv5x\\n프레임워크 VisualStudioCode/Python&Pytorch\\nGPU NvidiaRTX3080Ti\\nRAM 32GB\\n표7모델학습환경', metadata={'source': 'D:/여형구/test.pdf', 'page': 38})]\n",
      "\n",
      "### QUESTION:\n",
      "yolo 몇 모델을 사용했어?\n",
      "\n",
      "[/INST]\n",
      " \n",
      "### [INST]\n",
      "Instruction: Answer the question based on your knowledge.\n",
      "Here is context to help:\n",
      "\n",
      "[Document(page_content='- 31 -3)모델학습환경준비\\n본연구에서는적은양의데이터를가지고최선의결과를도출하기위해\\nYOLOv5의4가지모델중가장크고무거운YOLOv5x모델을사용하였다.\\n또한,학습파라미터를표6과같이설정하여학습을진행하였다.\\n종류 값\\nLearningRate(학습률) 0.01\\nBatchSize(배치크기) 8\\nEpochs(에폭) 50\\nOptimizer(옵티마이저) Adam\\nWeightDecay(가중치감소) 0.0005\\nMomentum(모멘텀) 0.937\\nInputImageSize(입력이미지크기) 640\\n표6모델학습파라미터\\n폭발물탐지모델은3080TiGPUCUDA환경에서VisualStudioCode를사\\n용하였고,메모리(RAM)의학습환경에서동작하였다.\\n종류 항목\\n모델 YOLOv5x\\n프레임워크 VisualStudioCode/\n"
     ]
    }
   ],
   "source": [
    "result = rag_chain.invoke(\"yolo 몇 모델을 사용했어?\")\n",
    "\n",
    "for i in result['context']:\n",
    "    print(f\"주어진 근거: {i.page_content} / 출처: {i.metadata['source']} - {i.metadata['page']} \\n\\n\")\n",
    "\n",
    "print(f\"\\n답변: {result['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "답변: \n",
      "### [INST]\n",
      "Instruction: Answer the question based on your knowledge.\n",
      "Here is context to help:\n",
      "\n",
      "[Document(page_content='- iv -<그림24>라벨링데이터예시 29\\n<그림25>VisualStudioCode환경에서YOLOv5모델학습30\\n<그림26>YOLOv5x모델학습결과(그림) 32\\n<그림27>학습모델의F1_curve(좌상),P_curve(우상),PR_curve(좌\\n하),R_curve(우하) 33\\n<그림28>True예시(좌),False중클래스만잘못예측한예시(우)34\\n<그림29>테스트데이터셋디텍팅결과(그림) 35\\n<그림30>실시간Detecting구현예시 36', metadata={'source': 'D:/여형구/test.pdf', 'page': 6}), Document(page_content='- 21 -3)YOLOv5[23]\\nYOLO는YouOnlyLookOnce의약자로객체탐지분야에서가장많이알\\n려진모델이다.객체탐지모델에는두가지접근방식이존재하는데예측결\\n과를도출하기까지몇번의단계를거치는지에따라One-Stage방식과\\nTwo-Stage방식으로나누어진다.YOLO는이름그대로One-Stage방식으로\\n객체를탐지하는모델로객체의경계상자(boundingbox)와클래스레이블\\n(classlabel)을직접예측하는모델이다.방식의특성상구현이간단하고속도\\n가빠르다는장점이있어YOLO모델이실시간객체탐지가가능하게한다.\\n그림16One-StagevsTwo-Stage방식비교16)\\nYOLO모델은이미지가입력되면우선가로세로를동일한그리드영역으\\n로나눈다.그후그리드영역에대해서어디에객체가존재하는지바운딩박\\n스와해당박스에대한신뢰도점수를예측한다.이와동시에객체의종류를\\n예측하는Classification작업이진행된다.최종적으로정확도가높은박스들만\\n남기고NMS(Non-Maximum Suppression)[22]알고리즘을이용해선별하여', metadata={'source': 'D:/여형구/test.pdf', 'page': 29}), Document(page_content='- 30 -3)모델학습환경준비\\n본연구에서는적은양의데이터를가지고최선의결과를도출하기위해\\nYOLOv5의4가지모델중가장크고무거운YOLOv5x모델을사용하였다.\\n또한,학습파라미터를표6과같이설정하여학습을진행하였다.\\n종류 값\\nLearningRate(학습률) 0.01\\nBatchSize(배치크기) 8\\nEpochs(에폭) 50\\nOptimizer(옵티마이저) Adam\\nWeightDecay(가중치감소) 0.0005\\nMomentum(모멘텀) 0.937\\nInputImageSize(입력이미지크기) 640\\n표6모델학습파라미터\\n폭발물탐지모델은3080TiGPUCUDA환경에서VisualStudioCode를사\\n용하였고,메모리(RAM)의학습환경에서동작하였다.\\n종류 항목\\n모델 YOLOv5x\\n프레임워크 VisualStudioCode/Python&Pytorch\\nGPU NvidiaRTX3080Ti\\nRAM 32GB\\n표7모델학습환경', metadata={'source': 'D:/여형구/test.pdf', 'page': 38})]\n",
      "\n",
      "### QUESTION:\n",
      "yolo 몇 모델을 사용했어?\n",
      "\n",
      "[/INST]\n",
      " \n",
      "### [INST]\n",
      "Instruction: Answer the question based on your knowledge.\n",
      "Here is context to help:\n",
      "\n",
      "[Document(page_content='- 31 -3)모델학습환경준비\\n본연구에서는적은양의데이터를가지고최선의결과를도출하기위해\\nYOLOv5의4가지모델중가장크고무거운YOLOv5x모델을사용하였다.\\n또한,학습파라미터를표6과같이설정하여학습을진행하였다.\\n종류 값\\nLearningRate(학습률) 0.01\\nBatchSize(배치크기) 8\\nEpochs(에폭) 50\\nOptimizer(옵티마이저) Adam\\nWeightDecay(가중치감소) 0.0005\\nMomentum(모멘텀) 0.937\\nInputImageSize(입력이미지크기) 640\\n표6모델학습파라미터\\n폭발물탐지모델은3080TiGPUCUDA환경에서VisualStudioCode를사\\n용하였고,메모리(RAM)의학습환경에서동작하였다.\\n종류 항목\\n모델 YOLOv5x\\n프레임워크 VisualStudioCode/\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n답변: {result['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
